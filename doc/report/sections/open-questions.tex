% !TEX root = ../main.tex

% open questions section

\section{Open questions and research directions}

So far, we have only considered S-estimators for linear regression. S-estimates for linear regression is defined as the minimizer of a robust estimate of scale. However, many other regression estimators are also defined as minimizers of some estimate of scale. Similarly, we may wish to consider replacing a non-robust estimate of scale with a robust estimate of scale to get a robust estimator. Such methods are very common and are also referred to as \textit{S-estimators}. In this section, we will discuss a new research direction, S-estimators for sparse linear regression. We will still consider the linear regression model defined in (\ref{eq:linRegr}).

Robust and sparse estimation for linear models with high multicollinearity is commonly required in fields such as chemometrics and bioinformatics. Improvements in sensor and computer technology has led to the number of variables $p$ to grow much larger than the number of observations $n$. However, the number of variables correlated with the response $s$ is still frequently very small relative to the number of observations, requiring sparsity. Sparse methods can improve estimator variance when $p/n$ is large and $s/n$ is small by selecting a subset of variables to include in the model.

The Least absolution shrinkage and selection operator (Lasso) \cite{tibsLasso} and Elastic Net \cite{hastie2005} are commonly used sparse regularized regression methods. Lasso penalizes the $L_1$ norm of the estimators to produce a sparse solution. However, Lasso is only able to select at most $n$ variables. Also, when groups of highly correlated variables are present in the data Lasso tends to select only one member of the group. To address these problems, Elastic Net penalizes a linear combination of the $L_1$ and the $L_2$ norms of the estimators. As a result, Elastic Net can be tuned to have different levels of sparsity and, unlike Lasso, can select more variables than $n$.

However, while both Lasso and Elastic Net address the issue of sparsity, they do not address the issue of outliers. In many applications, there can be sources of outlying data, including faulty equipment and mistakes during data transcription, making a robust method desirable. However, existing work on robust regularized regression has been limited. The first published highly robust regularized regression method was RLARS \cite{khan2007}, a modification of Least Angle Regression (LARS). This method replaces the sample correlation in the LARS method with a robust correlation. An extension of RLARS to include a variant of the Lasso modification to LARS is described in the thesis summary section. Later work by \cite{alfons2013} extended the Least Trimmed Squares estimator \cite{roussLTS} to include $L_1$ penalization of the estimators. This method is demonstrated to have good robustness properties, but can only be tuned for either high robustness or high efficiency under normality, but not both.

Recently, \cite{christidis2019split} proposed the Split Regularized Regression estimator (SplitReg). SplitReg includes a linear combination of the $L_1$ and $L_2$ penalties to encourage sparsity while also splitting variables between several models, penalizing similarity between models. Theoretical results \cite{christidis2018split} have shown SplitReg reduces estimator variance when data has some collinearity. Simulation studies and real-data applications have demonstrated improved predictive performance over the base estimator, Elastic Net, over a range of correlation structures, signal-to-noise ratios, and dimensionalities. This suggests that robust regularized regression method that splits variables between models like SplitReg is a good candidate to outperform existing robust regularized regression methods.

Therefore, the development of a robust split regularized regression estimator could offer significant improvements. The first step would be to develop a split regularized Elastic Net S-estimator by using a robust square scale function in the place of the sum of squares residual error. Consider the case where we have $G$ models in our estimator. The coefficients will be a $\bb = (\bb_1,\hdots,\bb_n)$ is a $p$x$G$ matrix, with the coefficients for each model $i \in {1,2,\hdots,G}$ being in the corresponding column of $\bb$. Let $\bb_g$ be column $g$ of $\bb$. The proposed estimator is defined as the solution to
\begin{equation}
    \argmin_{\bb \in \bbR^{pxG}} \sum_{g=1}^G \Bigg(\sigma(\mathbf{r}(\boldsymbol{\mu}, \bb_g))^2 + \lambda_s \Big( \frac{1}{2}(1 - \alpha)||\bb_g||^2_2 + \alpha ||\bb_g||_1)\Big) + \frac{\lambda_d}{2} \sum_{h \not=g}^G P_d(\bb_g, \bb_h) \Bigg)
\end{equation}
where $\sigma$ is an M-estimate of scale of the residuals, $\lambda_s$ scales the sparsity penalty and $\alpha$ tunes the combination of the $L_1$ and $L_2$ penalties, corresponding to elastic net, and $\lambda_d$ scales the diversity penalty and $P_d$ is diversity penalty, penalizing similarity models. This is the equivalent to summing the PENSE loss function \cite{freue2017pense} across the $G$ models with the addition of a diversity penalty. The most significant obstacle in this work would be the computational complexity, as both PENSE and SplitReg are already computationally intensive.

%For practical use, it would be necessary to improve the efficiency of the estimator by using S-estimator will be used to initialize a split M-estimator for regression. 

% Some examples are S-estimators for robust penalized splines \cite{tharmaratnam2010s}, Penalized S-estimators for Ridge Regression \cite{maronna2019robust}, and Penalized S-estimators for Elastic Net (PENSE) \cite{freue2017pense}. Each of these methods are defined as the minimizer of a robust scale and consequently gain robustness. The most common application of S-estimators is to provide the initial estimate of $\hat\bb$ and an estimate of scale $\hat\sigma$ for an M-estimator. The combination of both methods is called an MM-estimator \cite{maronna2019robust}. MM-estimators maintain the robustness of the S-estimator and gain efficiency from the M-estimator, producing a better estimate than each alone.